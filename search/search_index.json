{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is the documentation pages for my awesome kubernetes cluster :)  After being burnt way too many times from hardware failures, operating system  botched updates (and some times just plain boredom of maintaining services) I  was looking for an easy way of creating/maintaining/extending/deleting services.</p> <p>Kubernetes allows me to :</p> <ul> <li>to abstract my services from the hardware (so in case of a hardware failure, I can just restart them on another computer without too much effort)</li> <li>since the services are not dependent on the hardware, hopefully they will be much more reliable and much more scalable. </li> <li>document through code the requirements of each service and not having to write way too many ugly README files</li> <li>play with a new technology</li> <li>to have a safe sandbox to test new tools, without influencing the existing deployed services</li> </ul>"},{"location":"#who-is-this-for","title":"Who is this for?","text":"<p>For the strong of heart that are looking for new ideas. It is not meant as guide, but more as a starting step. I will not be providing any support, but any  bugs/questions you are welcome to create an issue at github (and if I have time I  will answer them, but no promises). Think of it as a collection of guides for  creating your own cluster and running your self-hosted services. I post them so if I ever have to start again from the begging, I will not have to do it  completely from start from the beggining.</p> <p>Another place that has some similar instructions is the  cookbook from Funky Penguin and the rpi4cluster. </p>"},{"location":"appendix/","title":"Apendix","text":"<p>These are instructions that were not fitting in any category</p>"},{"location":"appendix/#controloning-the-cluster-from-other-computers","title":"Controloning the cluster from other computers","text":""},{"location":"appendix/#installing-ansible","title":"Installing Ansible","text":"<p>Since I have a couple raspberries to manage, I will be using Ansible for that purpose. So before we start, we have to install ansible for managing the nodes. </p> <p>The instructions from the official page are:</p> <pre><code>sudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository --yes --update ppa:ansible/ansible\nsudo apt install ansible\n</code></pre>"},{"location":"appendix/#installing-kubectl-to-manage-the-cluster","title":"Installing kubectl to manage the cluster","text":"<p>Just install <code>kubectl</code> to the other computer and copy the necessary files</p> <pre><code>sudo snap install kubectl\n# Copy the config file to the local computer in .kube folder\n</code></pre>"},{"location":"appendix/#installing-helm-for-deploying-services","title":"Installing Helm for deploying services","text":"<p>Just follow the instructions from the official page</p>"},{"location":"appendix/#decrypting-kubectl-secrets","title":"Decrypting kubectl secrets","text":"<p>If we want to see the secret we can use: </p> <pre><code>kubectl -n longhorn-system get secret longhorn-crypto --template={{.data.CRYPTO_KEY_VALUE}} | base64 -d\nkubectl -n authentication get secret openldap-admin -o jsonpath=\"{.data.admin-password}\" | base64 -d\n</code></pre>"},{"location":"appendix/#building-the-documentations","title":"Building the documentations","text":"<p>The documentation pages were build with MkDocs  and use the Material theme</p> <p>What you need to build them:</p> <pre><code>pip3 install mkdocs\npip3 install mkdocs-material\npip3 install mkdocs-same-dir\nmkdocs serve\nmkdocs build\n</code></pre>"},{"location":"appendix/#listing-the-running-containers-on-a-node","title":"Listing the running containers on a node","text":"<pre><code>sudo k3s crictl ps\n</code></pre>"},{"location":"appendix/#removing-a-master-node","title":"Removing a master node","text":"<p>If a master node fails, we will have also to update the etcd nodes, to remove it from the etcd cluster</p> <pre><code>kubectl delete node &lt;name&gt;\n\nkubectl -n kube-system exec -it etcd-melpomene -- etcdctl --endpoints https://127.0.0.1:2379 \\\n    --cacert /etc/kubernetes/pki/etcd/ca.crt \\\n    --cert /etc/kubernetes/pki/etcd/server.crt \\\n    --key /etc/kubernetes/pki/etcd/server.key member list\n\nkubectl -n kube-system exec -it etcd-melpomene -- etcdctl --endpoints https://127.0.0.1:2379 \\\n    --cacert /etc/kubernetes/pki/etcd/ca.crt \\\n    --cert /etc/kubernetes/pki/etcd/server.crt \\\n    --key /etc/kubernetes/pki/etcd/server.key member remove &lt;node_id&gt;\n\n# Remove from kubeadm-config\n\nkubectl -n kube-system get cm kubeadm-config -o yaml &gt; /tmp/conf.yml\nmanually edit /tmp/conf.yml to remove the old server\nkubectl -n kube-system apply -f /tmp/conf.yml\n\n# https://github.com/k3s-io/k3s/issues/2732#issuecomment-749484037\n</code></pre>"},{"location":"appendix/#removing-failed-nodes-from-the-etcd-cluster","title":"Removing failed nodes from the etcd cluster","text":"<p>If a node fails it is not as simple to remove it and re-add it. Sometimes the failed node  will remain part of the etcd cluster. So will we have first to remove it from the etcd cluster.</p> <pre><code>kubectl delete node failed_node\n</code></pre> <p>If the master nodes are tainted we will have to remove the taint for the moment to be able to schedule a pod:</p> <pre><code>kubectl taint nodes other_master_node node-role.kubernetes.io/master=true:NoSchedule-\n</code></pre> <p>Then we can create a pod to manipulate the etcd cluster</p> <pre><code>\nkubectl run --rm --tty --stdin --image quay.io/coreos/etcd:v3.5.4 etcdctl --overrides='{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"spec\":{\"hostNetwork\":true,\"restartPolicy\":\"Never\",\"securityContext\":{\"runAsUser\":0,\"runAsGroup\":0},\"containers\":[{\"command\":[\"/bin/sh\"],\"image\":\"docker.io/rancher/coreos-etcd:v3.5.4-arm64\",\"name\":\"etcdctl\",\"stdin\":true,\"stdinOnce\":true,\"tty\":true,\"volumeMounts\":[{\"mountPath\":\"/var/lib/rancher\",\"name\":\"var-lib-rancher\"}]}],\"volumes\":[{\"name\":\"var-lib-rancher\",\"hostPath\":{\"path\":\"/var/lib/rancher\",\"type\":\"Directory\"}}],\"nodeSelector\":{\"node-role.kubernetes.io/etcd\":\"true\"}}}'\n\netcdctl --key /var/lib/rancher/k3s/server/tls/etcd/client.key --cert /var/lib/rancher/k3s/server/tls/etcd/client.crt --cacert /var/lib/rancher/k3s/server/tls/etcd/server-ca.crt member list\netcdctl --key /var/lib/rancher/k3s/server/tls/etcd/client.key --cert /var/lib/rancher/k3s/server/tls/etcd/client.crt --cacert /var/lib/rancher/k3s/server/tls/etcd/server-ca.crt member remove 1234567890ABCDEF\n</code></pre> <p>and the we re-taint the node</p> <pre><code>kubectl taint nodes other_master_node node-role.kubernetes.io/master=true:NoSchedule\n</code></pre> <p>Finally we can use the cluster_setup/setup_cluster.yml to re-populate the node.</p> <p>Note</p> <p>Remember to change the invetory if necessary</p>"},{"location":"appendix/#defrag-the-etcd-nodes","title":"Defrag the etcd nodes","text":"<pre><code>ansible amphitrite -b -m ansible.builtin.shell -a 'curl -L https://github.com/etcd-io/etcd/releases/download/v3.5.9/etcd-v3.5.9-linux-arm64.tar.gz -o /tmp/etcd-v3.5.9-linux-arm64.tar.gz'\nansible amphitrite -b -m ansible.builtin.shell -a 'tar xzvf /tmp/etcd-v3.5.9-linux-arm64.tar.gz -C /tmp --strip-components=1'\nansible amphitrite -b -m ansible.builtin.shell -a '/tmp/etcdctl version'\nansible amphitrite -b -m ansible.builtin.shell -a '/tmp/etcdctl --key /var/lib/rancher/k3s/server/tls/etcd/client.key --cert /var/lib/rancher/k3s/server/tls/etcd/client.crt --cacert /var/lib/rancher/k3s/server/tls/etcd/server-ca.crt member list' \nansible amphitrite -b -m ansible.builtin.shell -a '/tmp/etcdctl --key /var/lib/rancher/k3s/server/tls/etcd/client.key --cert /var/lib/rancher/k3s/server/tls/etcd/client.crt --cacert /var/lib/rancher/k3s/server/tls/etcd/server-ca.crt defrag --endpoints=[192.168.179.100:2379] ' \nansible amphitrite -b -m ansible.builtin.shell -a '/tmp/etcdctl --key /var/lib/rancher/k3s/server/tls/etcd/client.key --cert /var/lib/rancher/k3s/server/tls/etcd/client.crt --cacert /var/lib/rancher/k3s/server/tls/etcd/server-ca.crt defrag --cluster' \n</code></pre> <p>Resources: * https://gist.github.com/superseb/0c06164eef5a097c66e810fe91a9d408</p>"},{"location":"appendix/#updating-the-raspberry-pi-firmware","title":"Updating the raspberry pi firmware","text":"<p>If you have already an installed ubuntu on the Raspberry Pi you can execute:</p> <pre><code>sudo apt-get install rpi-eeprom\n\nsudo rpi-eeprom-update\nBCM2711 detected\nDedicated VL805 EEPROM detected\n*** UPDATE AVAILABLE ***\nBOOTLOADER: update available\nCURRENT: Mon Jul 15 12:59:55 UTC 2019 (1563195595)\n LATEST: Thu Sep  3 12:11:43 UTC 2020 (1599135103)\n FW DIR: /lib/firmware/raspberrypi/bootloader/default\nVL805: update available\nCURRENT: 00013701\n LATEST: 000138a1\n\nsudo rpi-eeprom-update -a\nsudo reboot\n</code></pre> <p>or via ansible:</p> <pre><code>ansible all -b -m package -a \"name=rpi-eeprom\"\nansible all -b -m shell -a \"rpi-eeprom-update\"\nansible all -b -m shell -a \"rpi-eeprom-update -a\"\nansible all -b -m reboot\n</code></pre> <p>If you have not already set-up the Raspberry Pi, then we will have to write first an sd card that will update the firmware</p>"},{"location":"appendix/#change-the-usb_msd_pwr_off_time","title":"Change the USB_MSD_PWR_OFF_TIME","text":"<p>Some times the USB devices are not re-attache after a reboot. </p> <pre><code>sudo apt-get install rpi-eeprom\n\nsudo rpi-eeprom-update\nBCM2711 detected\nDedicated VL805 EEPROM detected\n*** UPDATE AVAILABLE ***\nBOOTLOADER: update available\nCURRENT: Mon Jul 15 12:59:55 UTC 2019 (1563195595)\n LATEST: Thu Sep  3 12:11:43 UTC 2020 (1599135103)\n FW DIR: /lib/firmware/raspberrypi/bootloader/default\nVL805: update available\nCURRENT: 00013701\n LATEST: 000138a1\n\nsudo rpi-eeprom-config --edit\n\n# Add USB_MSD_PWR_OFF_TIME=0 in the end\nsudo reboot\n</code></pre>"},{"location":"appendix/#enabling-the-fans","title":"Enabling the fans","text":"<p>The raspberry pi 4 have the tendancy to get warm. So it is wise to have a fan. I am using PWN fans, so I can enable the fans based on the temperature. But to achieve that we need to install and compile some libraries.</p> <p>For convienience I am already including under the folder node_setup/fans the necessary binaries. But if we need to cross compiling the code follow the below instructions:</p> <pre><code>sudo apt-get install git build-essential gcc-aarch64-linux-gnu binutils-aarch64-linux-gnu\nwget http://www.airspayce.com/mikem/bcm2835/bcm2835-1.71.tar.gz\ntar zxvf bcm2835-1.71.tar.gz\ncd bcm2835-1.71\n./configure --build x86_64-pc-linux-gnu --host aarch64-linux-gnu \nmake\n\ncd ..\ngit clone https://gist.github.com/1c13096c4cd675f38405702e89e0c536.git\ncd 1c13096c4cd675f38405702e89e0c536\nmake CC=aarch64-linux-gnu-gcc LIBS=\"../bcm2835-1.71/src/libbcm2835.a -I../bcm2835-1.71/src/\"\n</code></pre> <p>Note</p> <p>To print the current temperature of the node, we can use ``` cpu=$(</p>"},{"location":"authorization/","title":"Authorization","text":"<p>We will use Authentik. Authentik allows us to define all the necessary permissions and have a second factor for the  user and to protect better our services.</p> <pre><code>kubectl create namespace authorization\n\nkubectl create secret generic authentik-generall --namespace authorization \\\n  --from-literal=secret-key=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \\\n  --from-literal=ak-admin-pass=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64)\n\nkubectl create secret generic authentik-postgresql --namespace authorization \\\n  --from-literal=postgres-password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \\\n  --from-literal=password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64)\n\nkubectl create secret generic authentik-redis --namespace authorization \\\n  --from-literal=redis-password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \n\nkubectl apply -f authentik-storage.yaml\n\nWe have to create first the certificates, so we can also use them inside \nauthentik. \n\nkubectl apply -f authentik-ingressroute.yaml\n\nhelm repo add authentik https://charts.goauthentik.io\nhelm repo update\nhelm install --namespace authorization authentik authentik/authentik -f values.yaml --version 2025.8.1\n</code></pre> <p>Note</p> <p>Initially the prometheus monitoring should not be activate until we deploy the  monitoring stack. Afterwards we can activate it.</p> <p>After the initialization we should also deploy the <code>recovery-email-verification.yaml</code> and the the two-factor login. Pay attention to force it.</p> <p>And now we can also make publicly from the interenet some dashboards, now that we can limit their access. To do that we will have first to define them inside the authentik app.</p> <p>We will have to create a new Provider (usually the proxy provider)  and a new Application for the proxy and the storage and then we  can deploy the ingress routes.</p> <pre><code>kubectl apply -f vpa.yaml\nkubectl apply -f sites/proxy-public.yaml\nkubectl apply -f sites/storage-public.yaml\n</code></pre> <p>For some of the proxy prover we need the extra middleware, because the we need to forward extra headers</p> <pre><code>kubectl apply -f authentik-middleware-remote-user-header.yaml\n</code></pre> <p>For the moment there is no good way of doing this via command line, but there might be progress in the future:</p> <ul> <li>https://github.com/goauthentik/helm/issues/127</li> </ul>"},{"location":"authorization/#usefull-commands","title":"Usefull commands","text":"<pre><code>kubectl -n authorization get secret authentik -o jsonpath=\"{.data.ak-admin-pass}\" | base64 -d\n\n# https://goauthentik.io/developer-docs/blueprints/export#global-export\nkubectl -n authorization exec -it authentik-worker-65c8449ccc-9nkdb -- bash\nak export_blueprint\n\n\n</code></pre>"},{"location":"authorization/#resources","title":"Resources:","text":"<ul> <li>https://docs.goauthentik.io/docs/troubleshooting/postgres/upgrade_kubernetes</li> <li>https://goauthentik.io/docs/installation/kubernetes</li> <li>https://goauthentik.io/docs/providers/proxy/</li> <li>https://goauthentik.io/docs/policies/expression?utm_source=authentik</li> <li>https://goauthentik.io/docs/flow/</li> </ul>"},{"location":"cluster_setup/","title":"Cluster Initialization","text":"<p>Now that are nodes are up and running we can start with the  initialization of the  k3s cluster. The scripts we have in ansible makes it easy. Just execute:</p> <pre><code>ansible-playbook cluster_setup/setup_cluster.yml\n</code></pre> <p>Note</p> <p>Flannel default network is not enrcypted. So the communication between the pods is unencrypted.  There is an experimental backend that supports Wireguard (https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#wireguard). I also tried with the experimental IPSec backend, but it did not work</p> <p>Once the cluster is initialized we can start controlling it using kubectl. But for that we have to install it to our local system</p> <pre><code>sudo snap install kubectl\n</code></pre> <p>We also have to copy /etc/rancher/k3s/k3s.yaml on your machine  located outside the cluster as ~/.kube/config. </p> <pre><code>ansible \"erato\" -b -m ansible.builtin.fetch -a 'src=/etc/rancher/k3s/k3s.yaml dest=~/.kube/config flat=yes'\n</code></pre> <p>Then replace \u201clocalhost\u201d with the IP or name of your K3s server. kubectl can now manage your K3s cluster.</p> <p>Note</p> <p>Unlike k8s, in k3s the master nodes are elible to run containers destined for workers as it does not  have the <code>node-role.kubernetes.io/master=true:NoSchedule</code>. To re-introduce it (if we see that the  master nodes are being influenced) we can execute  <code>kubectl taint nodes myserver node-role.kubernetes.io/master=true:NoSchedule</code> We can see the taints with <code>kubectl get nodes -o json | jq '.items[].spec.taints'</code></p>"},{"location":"cluster_setup/#resources","title":"Resources","text":"<ul> <li>https://rancher.com/docs/k3s/latest/en/cluster-access/#accessing-the-cluster-from-outside-with-kubectl</li> <li>https://rpi4cluster.com/k3s/k3s-kube-setting/</li> <li>https://github.com/k3s-io/k3s</li> <li>https://github.com/robipozzi/windfire-raspberry</li> <li>https://github.com/geerlingguy/raspberry-pi-dramble</li> <li>https://ikarus.sg/kubernetes-with-k3s/</li> <li>https://gist.github.com/LarsNieuwenhuizen/03c224e50871e123e4376f0518083cb1</li> </ul>"},{"location":"ddns/","title":"Dynamic IP","text":"<p>In my home network I do not have a static IP. So I have to be able to update my domain names to point to my dynamic IP. This is necessary to be able to access the services of my cluster from outside my network.</p> <p>Note</p> <p>It is also necessary to set up forwarding rules to your router, to forward the requests to the cluster.</p> <p>For the purpose I have created a docker image that is using my dns provider api to update my IP when it changes.</p> <p>Furthermore we are going to use external-dns to automatically update our dns.</p> <pre><code>kubectl create namespace general\nkubectl -n general create secret generic gandi --from-literal=API_KEY=GANDITOKEN\nkubectl apply -f ddns-anagno-me.yaml\nkubectl apply -f ddns-anagno-dev.yaml\n\nhelm repo add external-dns https://kubernetes-sigs.github.io/external-dns/ \nhelm repo update\nhelm install external-dns external-dns/external-dns -f values.yaml --namespace general --version 1.18.0\n\nkubect apply -f vpa.yml\n\nkubectl apply -f test.yaml\n# test that everything works\nkubectl delete -f test.yaml\n</code></pre> <p>Note</p> <p>Initially the prometheus monitoring should not be activate until we deploy the  monitoring stack. Afterwards we can activate it.</p>"},{"location":"ddns/#usefull-commands","title":"Usefull commands","text":"<pre><code>kubectl --namespace general logs -f -l \"app=ddns\"\n</code></pre> <p>Note</p> <p>Take a look at: https://docs.k8s-at-home.com/guides/dyndns/#creating-a-secret</p>"},{"location":"ddns/#resources","title":"Resources","text":"<ul> <li>https://github.com/kubernetes-sigs/external-dns</li> <li>https://github.com/kubernetes-sigs/external-dns/issues/1394#issuecomment-585228684</li> <li>https://github.com/kubernetes-sigs/external-dns/tree/438d06f3c45cf66d08945ae18d17e29c540d5c96/charts/external-dns</li> <li>https://grafana.com/grafana/dashboards/15038-external-dns/</li> </ul>"},{"location":"descheduler/","title":"Re-Balancing the pods","text":""},{"location":"descheduler/#descheduler","title":"Descheduler","text":"<p>To keep the load on the nodes balanced we will use the descheduler</p> <pre><code>helm repo add descheduler https://kubernetes-sigs.github.io/descheduler/\nhelm repo update\nhelm install descheduler descheduler/descheduler -f values.yaml --namespace kube-system --version v0.33.0\n</code></pre>"},{"location":"descheduler/#usefull-commands","title":"Usefull commands:","text":"<pre><code>kubectl -n kube-system logs -f -l \"app.kubernetes.io/instance=descheduler\"\nkubectl -n kube-system create job --from=cronjob/descheduler descheduler-im\n</code></pre>"},{"location":"descheduler/#resources","title":"Resources:","text":"<ul> <li>https://github.com/kubernetes-sigs/descheduler/blob/master/docs/user-guide.md</li> </ul>"},{"location":"github-runner/","title":"Self-hosted Github runner","text":"<p>For it will need to first create a github Personal Acces Token. For that follow the instructions  here.</p> <p>To store our files, we will use nextcloud</p> <pre><code>kubectl create namespace github-runner\n\nkubectl create secret generic controller-manager \\\n      -n github-runner \\\n      --from-literal=github_token=YOUR_ACCESS_TOKEN\n\nhelm repo add actions-runner-controller https://actions-runner-controller.github.io/actions-runner-controller\nhelm repo update\nhelm install --namespace github-runner github-controller actions-runner-controller/actions-runner-controller -f values.yaml --version 0.23.7\nkubectl apply -f runner.yaml\n\n\n</code></pre>"},{"location":"github-runner/#resources","title":"Resources:","text":"<ul> <li>https://medium.com/mossfinance/github-self-hosted-runners-on-kubernetes-with-actions-runner-controller-41e30c4cb76e</li> <li>https://github.com/actions/actions-runner-controller?tab=readme-ov-file#legacy-documentation</li> <li>https://www.velotio.com/engineering-blog/how-to-deploy-github-actions-self-hosted-runners-on-kubernetes</li> <li>https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/quickstart-for-actions-runner-controller</li> </ul>"},{"location":"hardware/","title":"Hardware","text":""},{"location":"hardware/#description","title":"Description","text":"<p>If you trying to do something similar, my suggestion is using Raspberry Pis 4 with  at least  4GB of RAM ( 8GB is better). The main limiting factor is memory, so the more memory you have the better.</p> <p>The hardware I am currently using is:</p> <ul> <li>4 x Raspberry Pi 4 Model B+ with 8GB Ram</li> <li>6 x Raspberry Pi 4 Model B+ with 4GB RAM</li> <li>10 x USB</li> <li>10 x Noctua NF-A4x20 5V PWM</li> <li>10 x DSLRKIT 5V 12V PoE HAT</li> <li>4 x Onvian USB 3.0 To Sata Adapter Converter Cable USB3.0</li> <li>1 x 5TB Seagate BarraCuda</li> <li>3 x 4TB WD Red Plus</li> <li>1 x StarTech 2-POST DESKTOP RACK - 8U</li> <li>1 x StarTech SERVER RACK SHELF VENTED 1U ST</li> <li>1 x UCTRONICS 19 inch 3U Rack Mount</li> <li>1 x Raspberry Pi 19 inch Rack Mount 1U</li> <li>1 x Roline Steckdosenleiste (Rack)</li> <li>1 x Netgear GS116PP: 16 Port Smart Switch</li> <li>1 x Buffalo 11ac 1166 Gigabit Wireless Dual Band Router</li> </ul> <p>You will also need jumper cables for the fans.</p> <p>Some images from the cluster:  </p> <p>I will not describe on how to set up DHCP, DNS, e.t.c servers because they are out of the scope of this project. Before starting I would suggest having a good convention  of naming your devices, because when (not not if -- do not doubt about the fact that  hardware will fail) the hardware fails it will be difficult to locate without good  naming conventions. I would also suggest of not hardcoding IPs, because if you have  to scale your network and everything is hardcoded, then you are out of luck.</p> <p>I would not suggest setting up the DHCP, DNS server on the kubernetes, because you will  have a chicken and egg problem. If there is a total failure (e.g. a power outage) you will not be able to restore the cluster. If the DHCP, DNS server is inside the  cluster, then the services will not be able to start because the cluster is not up,  and the cluster is not up, because there is no DHCP, DNS server. Currently I am using a DD-Wrt router for these services. In case the router fails, I always have on stand by an another router, that I can set up with the same parameters. Of course they  will be a downtime, but you have to choose your fights.</p>"},{"location":"library/","title":"Library","text":"<p>A small calibre server for accessing my books</p> <pre><code>kubectl create namespace library\nkubectl apply -f pvc.yaml\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\nkubectl apply -f ingressroute.yaml\nkubectl apply -f smb.yaml\n</code></pre> <p>Note</p> <p>The first time configuration must be done automatically. I could not locate a way to automate it. Furthermore the proxy authentication is not perfect. We have to first create manually the users in the database of  calibre-web Furthermore, for the opds catalogue to work, we have to activate the anonymous browsing from the menu. Probably a bug in the calibre-web. Theoretically it should not be a problem, since we are behind our proxy and authentication</p>"},{"location":"library/#usefull-commands","title":"Usefull commands","text":"<pre><code>nohup rsync -avh --progress --bwlimit=10000 /home/anagno/Books/ . &gt; nohup2.out 2&gt;nohup2.err &lt; /dev/null &amp;\nmount -t cifs -o username=anagno,password=anagno //192.168.179.236/books/ smb_books/\nsudo mount -t cifs -o username=anagno,password=anagno //192.168.179.236/books/ smb_books/\ncd smb_books/\nsudo rsync -avh --progress --bwlimit=10000 /home/anagno/Books/ .\nsudo umount smb_books \nrm -R smb_books/\n</code></pre>"},{"location":"library/#resources","title":"Resources:","text":"<ul> <li>https://github.com/janeczku/calibre-web/wiki/Setup-Reverse-Proxy/#reverse-proxy</li> <li>https://superuser.com/questions/1229146/rsync-for-syncing-4tb-from-smb-to-usb-volume-hostnameproblem</li> </ul>"},{"location":"load_balancer/","title":"Load balancer","text":""},{"location":"load_balancer/#metallb","title":"Metallb","text":"<p>For the load balancer we are using the metallb. Theoretically we could have used the integratetd one from the k3s, but the  integrated one is not able to handle virtual IPs.  Alternative we could have used the kube-vip but the provided controller from kube-vip is not mature enough. If we ever decide to use kube-vip then we will have  to update the kube-vip daemon set and activate the <code>--service</code> parameter.</p> <p>Metallb provides a helm chart. So the installation is quite simple:</p> <pre><code>helm repo add metallb https://metallb.github.io/metallb\nkubectl create namespace load-balancer\nhelm install --namespace load-balancer load-balancer metallb/metallb -f values.yaml --version 0.15.2\n# Wait for the full deployment of the services\nkubectl apply -f IPAddressPool.yaml\nkubectl apply -f vpa.yaml\n</code></pre> <p>The <code>values.yaml</code> I used is:</p> <pre><code># https://github.com/metallb/metallb/blob/main/charts/metallb/values.yaml\n\nprometheus:\n  serviceAccount: \"prometheus-k8s\"\n  namespace: \"monitoring\"\n  podMonitor:\n    enabled: true\n\ncontroller:\n  image:\n    pullPolicy: IfNotPresent\n\nspeaker:\n  image:\n    pullPolicy: IfNotPresent\n</code></pre> <p>Note</p> <p>Initially the prometheus monitoring should not be activate until we deploy the  monitoring stack. Afterwards we can activate it.</p>"},{"location":"load_balancer/#validation","title":"Validation","text":"<p>To make sure that the assignment of the IPs is working, we will create a small testing service </p> <pre><code>kubectl create namespace kube-verify\nkubectl apply -f verify.yaml\n</code></pre> <p>With <code>verify.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-verify\n  namespace: kube-verify\n  labels:\n    app: kube-verify\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kube-verify\n  template:\n    metadata:\n      labels:\n        app: kube-verify\n    spec:\n      containers:\n      - name: nginx\n        image: quay.io/clcollins/kube-verify:01\n        ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-verify\n  namespace: kube-verify\nspec:\n  selector:\n    app: kube-verify\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  type: LoadBalancer\n  loadBalancerIP: 192.168.179.249\n\n\n</code></pre> <p>The test service should be now availalbe in the IP we specified in verify. Once we make certain that everything is working we can deleting the validation service:</p> <pre><code>kubectl delete -f verify.yaml\nkubectl delete namespace kube-verify\n</code></pre>"},{"location":"load_balancer/#userfull-commands","title":"Userfull commands:","text":"<p>To get all the assigned IPs we just execute:</p> <pre><code>kubectl get services -o wide --all-namespaces | grep --color=never -E 'LoadBalancer|NAMESPACE'\n</code></pre>"},{"location":"load_balancer/#resources","title":"Resources:","text":"<ul> <li>https://opensource.com/article/20/7/homelab-metallb</li> <li>https://www.devtech101.com/2019/02/23/using-metallb-and-traefik-load-balancing-for-your-bare-metal-kubernetes-cluster-part-1/</li> <li>https://metallb.universe.tf/configuration/</li> <li>https://github.com/metallb/metallb/issues/308</li> <li>https://opensource.com/article/20/7/homelab-metallb</li> </ul> <p>https://github.com/inlets/inlets-operator</p> <p>IPV6</p> <p>https://kubernetes-sigs.github.io/external-dns/v0.14.0/sources/service/#clusterip-headless https://kubernetes-sigs.github.io/external-dns/v0.14.0/tutorials/traefik-proxy/#manifest-for-clusters-with-rbac-enabled https://kubernetes-sigs.github.io/external-dns/v0.14.0/tutorials/hostport/ https://metallb.universe.tf/troubleshooting/ kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'</p> <p>https://docs.k3s.io/cli/server#networking </p> <p>--cluster-cidr=10.42.0.0/16,fd42::/56 --service-cidr=10.43.0.0/16,fd43::/112</p> <p>https://github.com/kubernetes/kubernetes/issues/81677#issuecomment-524351347</p> <p>https://github.com/kubernetes/kubernetes/issues/111671</p> <p>https://github.com/k3s-io/docs/pull/103/files</p>"},{"location":"monitoring/","title":"Monitoring","text":"<p>Without a good monitoring system, we will not be able to find the problems that our  cluster has. So we will be using the prometheus stack to monitor the whole cluster:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo add stable https://charts.helm.sh/stable\nhelm repo update\n\nkubectl create namespace monitoring\n\n\n# Follow the instructions for goauthentik\n# https://goauthentik.io/integrations/services/grafana/\n\nkubectl create secret generic authentik-secret --namespace monitoring \\\n  --from-literal=client_id=ID_FROM_AUTHENTIK \\\n  --from-literal=client_secret=SECRET_FROM_AUTHENTIK\n\nhelm install --namespace monitoring monitoring prometheus-community/kube-prometheus-stack -f values.yaml \\\n    --version v77.1.0 --set grafana.adminPassword=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64)\nkubectl apply -f monitoring-ingress-public.yaml\nkubectl apply -f vpa.yaml\n\n\n# https://github.com/grafana/helm-charts/issues/3300\nhelm install --namespace monitoring loki grafana/loki-stack -f loki-values.yaml --version 2.10.2\n\nhelm install --namespace monitoring event-explorter bitnami/kubernetes-event-exporter -f event-exporter-values.yaml --version 2.9.3\n\n</code></pre> <p>To get the password:</p> <pre><code>kubectl -n monitoring get secret monitoring-grafana -o jsonpath=\"{.data.admin-password}\" | base64 -d\n</code></pre> <p>Now we can start adding some more dashboards</p> <ul> <li>Monitoring the storage cluster:</li> </ul> <pre><code>kubectl apply -f storage/service-monitor.yaml\nkubectl apply -f storage/storage-dashboard.yaml\n</code></pre> <ul> <li>Monitoring the proxy:</li> </ul> <pre><code>kubectl apply -f proxy/traefik-dashboard-service.yaml\nkubectl apply -f proxy/traefik-service-monitor.yaml\nkubectl apply -f proxy/traefik-dashboard.yaml\nkubectl apply -f proxy/traefik-dashboard-loki.yaml\n</code></pre> <ul> <li>General dashboards:</li> </ul> <pre><code>kubectl apply -f dashboards/alerts-summary-dashboard.yaml\nkubectl apply -f dashboards/alerts-dashboard.yaml\nkubectl apply -f dashboards/cluster-details-dashboard.yaml\nkubectl apply -f dashboards/cluster-details-namespaces.yaml\nkubectl apply -f dashboards/cluster-details-nodes.yaml\nkubectl apply -f dashboards/volumes-dashboard.yaml\nkubectl apply -f dashboards/node-exporter.yaml\nkubectl apply -f dashboards/vpa-dashboard.yaml\nkubectl apply -f dashboards/hpa-dashboard.yaml\nkubectl apply -f dashboards/event-exporter.yaml\nkubectl apply -f dashboards/loki-search.yaml\n</code></pre> <ul> <li>Load-balancer dashboard (if it has been activated in the helm chart): </li> </ul> <pre><code>kubectl apply -f dashboards/metallb-dashboard.yaml\n</code></pre> <ul> <li>Certificates dashboard (if it has been activated in the helm chart): </li> </ul> <pre><code>kubectl apply -f dashboards/cert-manager-dashboard.yaml\n</code></pre> <ul> <li>External-dns dashboard (if it has been activated in the helm chart): </li> </ul> <pre><code>kubectl apply -f dashboards/externa-dns-dashboard.yml\n</code></pre> <ul> <li>Authorization dashboard (if it has been activated in the helm chart): </li> </ul> <pre><code>kubectl apply -f dashboards/authentik-dashboard.yaml\n</code></pre>"},{"location":"monitoring/#usefull-commands","title":"Usefull commands:","text":"<pre><code>kubectl -n monitoring port-forward service/monitoring-kube-prometheus-prometheus 9090:9090\n</code></pre> <p>TODO add as dashboard  https://grafana.com/grafana/dashboards/15760-kubernetes-views-pods/ https://grafana.com/grafana/dashboards/21410-kubernetes-overview/</p>"},{"location":"monitoring/#resources","title":"Resources","text":"<ul> <li>https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack</li> <li> <p>https://traefik.io/blog/capture-traefik-metrics-for-apps-on-kubernetes-with-prometheus/</p> </li> <li> <p>https://grafana.com/grafana/dashboards/15398</p> </li> <li>https://grafana.com/grafana/dashboards/15761</li> </ul> <p>Important issues: * https://github.com/kubernetes/kube-state-metrics/pull/1237</p> <p>https://github.com/kubernetes/kube-state-metrics/issues/2041 https://github.com/prometheus-community/helm-charts/blob/c8d79dbef2b93fad69e458da2ecd09920fb786a7/charts/kube-state-metrics/values.yaml#L338</p>"},{"location":"nextcloud/","title":"Nextcloud","text":"<p>To store our files, we will use nextcloud</p> <pre><code>kubectl create namespace cyberlocker\n\nkubectl create secret generic nextcloud --namespace cyberlocker \\\n  --from-literal=admin-username=caretaker \\\n  --from-literal=admin-password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \\\n  --from-literal=serverinfo_token=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \\\n  --from-literal=smtp_username= \\\n  --from-literal=smtp_password= \\\n  --from-literal=smtp_host=\n\nkubectl create secret generic nextcloud-postgresql-database --namespace cyberlocker \\\n  --from-literal=postgres-password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \\\n  --from-literal=password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64)\n\nkubectl create secret generic nextcloud-redis --namespace cyberlocker \\\n  --from-literal=redis-password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \n\nkubectl apply -f storage.yaml\n\n\nhelm repo add nextcloud https://nextcloud.github.io/helm/\nhelm repo update\nhelm install cyberlocker nextcloud/nextcloud -f values.yaml --namespace cyberlocker --version 7.0.2\n\nkubectl apply -f ingressroute.yaml\nkubectl apply -f vpa.yaml\n</code></pre> <p>To set up the nextcloud with oidc from goauthentik follow the instructions from:</p> <ul> <li>https://blog.cubieserver.de/2022/complete-guide-to-nextcloud-oidc-authentication-with-authentik/</li> </ul> <p>Usefull commands:</p> <pre><code>kubectl -n cyberlocker exec -it box-nextcloud-88858c579-mq7sv -- /bin/bash\nsu -s /bin/bash www-data\nphp occ app:update --all\nphp occ maintenance:mode --off \nphp occ config:system:set overwrite.cli.url --value=\"https://cyberlocker.anagno.dev\"\n\nkubectl -n cyberlocker get secret nextcloud -o jsonpath=\"{.data.admin-password}\" | base64 -d\nkubectl -n cyberlocker exec -it cyberlocker-postgresql-0 -- psql -d nextcloud -U nextcloud\n</code></pre> <p>I will have to update the deployment to include </p> <pre><code>dnsConfig:\n  options:\n    - name: ndots\n      value: \"1\"\n</code></pre> <p>For the deployment to have access to the internet</p> <p>https://grafana.com/grafana/dashboards/17821-nextcloud-log/ https://okxo.de/monitor-your-nextcloud-logs-for-suspicious-activities/ https://voidquark.com/blog/parsing-nextcloud-audit-logs-with-grafana-loki/</p> <p>https://github.com/grafana/helm-charts/blob/main/charts/loki-stack/values.yaml</p>"},{"location":"node_setup/","title":"Node preparation","text":"<p>Before even starting with setting up the cluster, we should prepare the  nodes. These steps should be repeated for all the nodes of the cluster.</p>"},{"location":"node_setup/#basic-settings-on-each-node","title":"Basic settings on each node","text":"<p>I am going to use Ansible for most tasks, but before we are able to do that we  will have to a couple of manual tasks.</p>"},{"location":"node_setup/#preparing-the-hosts-file-of-ansible","title":"Preparing the hosts file of Ansible","text":"<p>In my internal logic of the ansible scripts, I have 3 groups:</p> <ul> <li>masters -- which eventually will be the master nodes of the k3s</li> <li>workers -- which eventually will be the \"worker\" nodes of the k3s</li> <li>disks -- special nodes that also have an hhd/ssd disk for writing bigger data</li> </ul> <p>Note</p> <p>In the disks group we are also defining the path of the mounted disks.   It is usefull when we will be setting up our storage</p> <p>So your hosts file will look like something:</p> <pre><code>all:\n  hosts:\n    node_1:\n      ansible_host: whatever\n      ansible_user: whatever\n      ansible_ssh_pass: whatever\n    node_2:\n      ansible_host: whatever\n      ansible_user: whatever\n      ansible_ssh_pass: whatever\n    node_3:\n      ansible_host: whatever\n      ansible_user: whatever\n      ansible_ssh_pass: whatever\n  children:\n    masters:\n      hosts:\n        node_1:\n    workers:\n      hosts:\n        node_2:\n        node_3:\n    disks:\n      hosts:\n        node_3:\n          disk_path: /dev/sda\n          UUID: whatever\n        node_4:\n          disk_path: /dev/sdb\n          UUID: whatever\n</code></pre> <p>Using the <code>ansible_ssh_pass</code> variable is not the best idea, but if you insist it is better to encrypt the data by using the ansible-vault. This can be achieved by</p> <pre><code>ansible-vault encrypt_string 'my_strong_password' --name 'ansible_ssh_pass' \n</code></pre> <p>Note</p> <p>The above command does not pass the vault_password_file because it is  defined in the ansible.cfg</p>"},{"location":"node_setup/#operating-system","title":"Operating system","text":"<p>The most simple OS to use is Ubuntu.  You should use a 64bit image, because generally many of the images we are using are  only available for aarch64 not the arm4vl (which is the 32 bit version).</p> <p>Ubuntu is bootable from a usb from Ubuntu 20.10 So we will have to make it bootable:</p>"},{"location":"node_setup/#upgrade-the-firmware-if-necessary","title":"Upgrade the firmware (if necessary)","text":"<p>We are going to use USB disk, instead of SD cards. But the firmware of Raspberry Pi support the boot from the USB after 2021. So make certain the firmware is updated. Check the appendix for details on how to update the firmware</p>"},{"location":"node_setup/#set-up-the-password-and-hostname","title":"Set up the password and hostname","text":"<p>We have to SSH log in manually to every node, and change the password when asked (use  strong passwords). So for each node we will have the username ubuntu and the password you used.</p> <p>After that step we can restart the node and everything else will be done via ansible</p>"},{"location":"node_setup/#first-setup-of-via-ansible","title":"First setup of via ansible","text":"<p>The scripts we have in ansible makes it easy. Just execute:</p> <pre><code>ansible-playbook node_setup/setup_node.yml\n</code></pre> <p>To set-up a signle node we can execute:</p> <pre><code>ansible-playbook node_setup/setup_node.yml --limit \"node\"\n</code></pre>"},{"location":"node_setup/#resources","title":"Resources","text":"<ul> <li>https://opensource.com/article/20/6/kubernetes-raspberry-pi</li> <li>https://askubuntu.com/questions/1189480/raspberry-pi-4-ubuntu-19-10-cannot-enable-cgroup-memory-at-boostrap</li> <li>https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip</li> <li>https://thebsdbox.co.uk/2020/01/02/Designing-Building-HA-bare-metal-Kubernetes-cluster/#Networking-load-balancing</li> <li>https://tanzu.vmware.com/content/blog/exploring-kube-apiserver-load-balancers-for-on-premises-kubernetes-clusters</li> <li>https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step-Part5/</li> <li>https://stackoverflow.com/questions/56634139/how-to-use-the-master-node-as-worker-node-in-kubernetes-cluster</li> <li>https://www.shogan.co.uk/kubernetes/building-a-pi-kubernetes-cluster-part-3-worker-nodes-and-metallb/</li> <li>https://opensource.com/article/20/7/homelab-metallb</li> <li>https://blog.lzoledziewski.pl/2020/06/01/noctua-5v-pwm-fan-rpi-4/</li> <li>https://blog.driftking.tw/en/2019/11/Using-Raspberry-Pi-to-Control-a-PWM-Fan-and-Monitor-its-Speed/</li> <li>https://www.raspberrypi.org/forums/viewtopic.php?f=63&amp;t=244194</li> <li>https://gist.github.com/alwynallan/1c13096c4cd675f38405702e89e0c536</li> <li>https://github.com/geerlingguy/raspberry-pi-dramble</li> <li>https://github.com/k3s-io/k3s</li> </ul>"},{"location":"portainer/","title":"Portainer","text":"<p>To administrate some aspects of the cluster we use portainer</p> <pre><code>kubectl create namespace pantheon\nkubectl apply -f storage.yaml\n\n\nhelm repo add portainer https://portainer.github.io/k8s/\nhelm repo update\nhelm install portainer portainer/portainer  -f values.yaml --namespace pantheon --version 2.33.1\n\nkubectl apply -f ingressroute.yaml\nkubectl apply -f vpa.yaml\n</code></pre> <p>To set up the nextcloud with oidc from goauthentik follow the instructions from:</p> <ul> <li>https://goauthentik.io/integrations/services/portainer/</li> </ul>"},{"location":"proxy/","title":"Reverse Proxy","text":"<p>Since we only have a single public IP, if we want to set up multiple public services we  will need a reverse proxy to redirect the requests to the correct services. Our proxy will also handle the ssl termination, to simplify the set up of the services. </p> <p>For managing our certificates we will be using <code>cert-manager</code> and as a proxy we  will be using <code>Traefik</code>.</p> <p>Note</p> <p>Traefik is the default proxy from k3s, but to have more granular control we are disabling it and we are going to deploy it as Helm chart</p>"},{"location":"proxy/#traefik","title":"Traefik","text":"<p>Similarly, to deploy Traefik we have to execute:</p> <pre><code>kubectl create namespace proxy\nhelm repo add traefik https://helm.traefik.io/traefik\nhelm repo update\nhelm install -n proxy traefik traefik/traefik -f traefik-values.yaml --version 37.0.0\n</code></pre> <p>Note</p> <p>In the logs we might have a warning and we might have to increase the rmeem_max <code>ansible all -b -m shell -a \"sysctl -w net.core.rmem_max=2500000\"</code>. To retrieve the logs do <code>kubectl -n proxy logs -f -l \"app.kubernetes.io/name=traefik\"</code></p> <pre><code># https://github.com/traefik/traefik-helm-chart/blob/master/traefik/values.yaml\n\nimage:\n  #tag: \"v2.11.0\"\n  pullPolicy: IfNotPresent\n\ndeployment:\n  enabled: true\n  kind: Deployment\n  replicas: 2\n\ningressClass:\n  enabled: true\n  isDefaultClass: true\n\n# We will create our IngressRoute for the dashboard\ningressRoute:\n  dashboard:\n    enabled: False\n\nproviders:\n  kubernetesCRD:\n    enabled: true\n    allowCrossNamespace: true\n    allowExternalNameServices: true\n  kubernetesIngress:\n    enabled: true\n\n\nadditionalArguments:\n  # Used for the generation of the certificates\n  - \"--providers.kubernetesIngress.ingressClass=traefik-cert-manager\"\n  - \"--providers.kubernetesingress.allowexternalnameservices=true\"\n\nlogs:\n  general:\n    level: INFO\n    format: json\n  access:\n    enabled: true\n    format: json\n\nmetrics:\n  prometheus:\n    entryPoint: metrics\n    addEntryPointsLabels: true\n    addRoutersLabels: true\n    addServicesLabels: true\n    manualRouting: false\n    service:\n      enabled: false\n\n\n# TLS Options are created as TLSOption CRDs\n# https://doc.traefik.io/traefik/https/tls/#tls-options\n# Example:\ntlsOptions:\n  default:\n    sniStrict: true\n    preferServerCipherSuites: true\n    cipherSuites:\n      - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n      - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n      - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\n    curvePreferences:\n      - CurveP521\n      - CurveP384\n\nports:\n  websecure:\n    http3:\n      enabled: true\n  metrics:\n    # It has to be the same with the traefik-dashboard-service.yaml in monitoring\n    port: 9100\n\nservice:\n  enabled: true\n  type: LoadBalancer\n  spec: \n    externalTrafficPolicy: Local\n    loadBalancerIP: \"192.168.179.240\"\n\nresources:\n  requests:\n    cpu: \"250m\"\n    memory: \"100Mi\"\n\nautoscaling:\n  enabled: true\n  minReplicas: 1\n  maxReplicas: 5\n\n</code></pre> <p>Now that our proxy is running we can define some Middlewares to simplify the  deployment of services:</p> <pre><code>kubectl apply -f https_redirect.yaml\nkubectl apply -f security_headers.yaml\n</code></pre> <p>Traefik will be my main proxy that will forward requests to other services. So  we also deploy these requirements:</p> <pre><code>kubectl apply -f grigoris_proxy.yaml\nkubectl apply -f box_proxy.yaml\nkubectl apply -f traefik-dashboard.yaml\n</code></pre>"},{"location":"proxy/#cert-manager","title":"Cert-manager","text":"<p>Since we want high availability, we need also to handle our certificates in a way that can be  hanlded from traefik. Traefik offers this support in the enteprise version, but since I am  cheap, I will use an another manager (i.e. cert-manager)</p> <pre><code>kubectl create namespace cert-manager\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install cert-manager jetstack/cert-manager --namespace cert-manager -f cert-values.yaml --version v1.18.2\nkubectl apply -f cert-vpa.yaml\n</code></pre> <p>With <code>cert-values.yaml</code>:</p> <pre><code>#https://github.com/jetstack/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml\n\ncrds:\n  enabled: true\n\nreplicaCount: 1\n\n#https://github.com/jetstack/cert-manager/issues/959\npodDnsPolicy: \"Default\"\n\nprometheus:\n  enabled: true\n  servicemonitor:\n    enabled: true\n\n</code></pre> <p>Note</p> <p>Initially the prometheus monitoring should not be activate until we deploy the  monitoring stack. Afterwards we can activate it.</p> <p>Now that the <code>cert-manager</code> is running we can create our certificate issuers:</p> <pre><code>kubectl apply -f self_signed.yaml\nkubectl apply -f letenrcypt_stagging.yaml\nkubectl apply -f letenrcypt.yaml\n</code></pre> <p>Note</p> <p>We are using <code>traefik-cert-manager</code> as name for the traefik ingress. That we will have to specify it when we deploy the traefik</p>"},{"location":"proxy/#testing-that-everything-works","title":"Testing that everything works","text":"<p>To test that everything works, we can deploy a testing whoami service:</p> <pre><code>kubectl apply -f whoami.yaml\n# Check that everything works and the delete the service\nkubectl delete -f whoami.yaml\n# Forcing renew. Good practice when updating the cert-manager to make sure that\n# everything still works\nkubectl cert-manager status certificate login.anagno.me -n authentication\nkubectl cert-manager renew login.anagno.me -n authentication\n</code></pre> <p>Resources:</p> <ul> <li>https://medium.com/dev-genius/setup-traefik-v2-for-ha-on-kubernetes-20311204fa6f</li> <li>https://traefik.io/blog/install-and-configure-traefik-with-helm/</li> <li>https://kubernetes.github.io/ingress-nginx/deploy/baremetal/</li> <li>https://www.thebookofjoel.com/k3s-cert-manager-letsencrypt</li> <li>https://cert-manager.io/docs/usage/kubectl-plugin/</li> </ul>"},{"location":"rss/","title":"Rss","text":"<p>A small rss server for keeping up with the rss feeds</p> <pre><code>kubectl create namespace news\nkubectl apply -f pvc.yaml\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\nkubectl apply -f ingressroute.yaml\n</code></pre>"},{"location":"rss/#usefull-commands","title":"Usefull commands","text":"<pre><code>kubectl -n news exec -it freshrss-88b8985bb-5lrgt -- bash\necho \"&lt;?php phpinfo();\" &gt;&gt; p/i/phpinfo.php\n\n\n# To move the subscriptions \nscp ubuntu@old_freshrss:/home/ubuntu/freshrss/www/freshrss/data/users/anagno/db.sqlite db.sqlite\nkubectl cp db.sqlite news/freshrss-56d767678c-md6h9:/var/www/FreshRSS/data/users/anagno/db.sqlite\n\n\n./cli/do-install.php --default_user caretaker --auth_type http_auth --environment production --base_url https://news.anagno.dev --language en --title FreshRSS --api_enabled --db-type sqlite\n\n# To copy a local config.php to the container\nkubectl cp config.php news/freshrss-7495bcb86b-5wtd7:/var/www/FreshRSS/data/config.php\n</code></pre> <p>Note</p> <p>The first time configuration must be done automatically. I could not locate a way to automate it. Remember also to activate the api</p>"},{"location":"rss/#resources","title":"Resources:","text":"<ul> <li>https://github.com/t13a/helm-chart-freshrss/blob/master/values.yaml</li> <li>https://docs.syseleven.de/metakube/de/tutorials/sharing-volumes-between-pods</li> <li>https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/</li> </ul>"},{"location":"secrets/","title":"Sharing secrets accross namespaces","text":""},{"location":"secrets/#kubernetes-replicator","title":"Kubernetes replicator","text":"<p>Kubernets does not share secrets accross namespaces. But some times it is  usefull to be able to share secrets accross namespaces. For that purpose  we use the kubernetes-replicator</p> <pre><code>helm repo add mittwald https://helm.mittwald.de\nhelm repo update\nhelm install kubernetes-replicator mittwald/kubernetes-replicator -f values.yaml --version v2.10.2 --namespace kube-system\nkubectl apply -f vpa.yaml\n</code></pre> <p>The <code>values.yaml</code> I used is:</p> <pre><code># https://github.com/mittwald/kubernetes-replicator/blob/master/deploy/helm-chart/kubernetes-replicator/values.yaml\n\nreplicationEnabled:\n  secrets: true\n  configMaps: true\n  roles: true\n  roleBindings: true\n  serviceAccounts: true\n\n</code></pre> <p>After the service is deployed we can create secrets in any namespace that can be synced accross namespaces:</p> <pre><code>kubectl -n general create secret generic no-reply-mail --from-literal=password=PASSWORD\nkubectl -n general annotate secret no-reply-mail replicator.v1.mittwald.de/replicate-to=\"*\"\n</code></pre>"},{"location":"secrets/#resources","title":"Resources","text":"<ul> <li>https://appscode.com/products/kubed/v0.12.0/guides/config-syncer/intra-cluster/</li> </ul>"},{"location":"storage/","title":"Storage","text":"<p>Stateless applications/services are fun, but eventually we would  like to store some state/data. The options we have are:</p> <ul> <li>Rook/Ceph: I have tested rook and the options are and features  are really extensive and very good. It is quite stable, but we are facing the limitations of our platform. Raspberry Pi is an arm64 platform. Ceph does support arm64, but some other usefull features (e.g. running NFS  endpoints) did not have yet arm64 images. Furthermore it needs a lot of  CPU and memory. To deploy a fully functional ceph cluster we need at  least 10 Raspberry Pi (and they have to be raspberry pi 4 with at  least 4GB of RAM). So technically it is possible and I have done it, but  the headaches are too many. Do not get me wrong. I am pushing what is  feasible by deploying it to Raspberry Pies and it still worked. So kudos to Ceph. But probably it is an overcomplicated solution for what I want to do. </li> <li>NFS: We can use an external nfs storage server. Probably this one will  work like charm, but it is a single point of failure. </li> <li>Longhorn: This is another native Kubernetes storage, which seems quite  promising. They only disadvantage is that it does not support  sharding. So we will be  limiting our selfes to the maximum size of our hard drives in the cluster. For my current use cases, it is more that fine. Furthermore it is supported  from k3s. So we will go with it and see what happens.</li> </ul>"},{"location":"storage/#preparing-the-nodes","title":"Preparing the nodes","text":"<p>Before we are able to install longhorn, we have to prepare the nodes  and mount the disks. To be able to do that we will have to able to find out where the devices are located. So we execute:</p> <pre><code>ansible disks -b -m ansible.builtin.shell -a 'blkid'\n</code></pre> <p>We will get something like:</p> <pre><code>/dev/sda: UUID=\"blah-blah-...-blah\" TYPE=\"LVM2_member\"\n</code></pre> <p>So complete these details in our hosts file in the section of the disks. E.g.:</p> <pre><code>    disks:\n      hosts:\n        node_1:\n          disk_path: /dev/sda\n</code></pre> <p>Then we can execute our playbook</p> <pre><code>ansible-galaxy collection install ansible.posix\nansible-galaxy collection install community.general\nansible-playbook storage/setup_storage.yml\n</code></pre> <p>Warning</p> <p>I have no idea why, but if I connect the Sata to USB3 cable I have to the USB3 ports, after a reboot the drive will not re-connect. To reconnect I have to power completely the raspberry pi (unplug and replug the PoE cable). Needs investigation.</p> <p>Note</p> <p>Be carefull to use the correct partition</p> <p>Note</p> <p>I used to have ceph installed on the disks and to clean I have first to execute:</p> <p><code>ansible disks -b -m ansible.builtin.shell -a 'sgdisk --zap-all /dev/sdb' ansible disks -b -m ansible.builtin.shell -a 'sudo dd if=/dev/zero of=/dev/sdb bs=1M count=100 oflag=direct,dsync' ansible disks -b -m ansible.builtin.shell -a 'ls /dev/mapper/ceph-* | xargs -I% -- sudo dmsetup remove %' ansible disks -b -m ansible.builtin.shell -a 'sudo rm -rf /dev/ceph-*'</code></p>"},{"location":"storage/#installing-longhorn","title":"Installing longhorn","text":"<pre><code># We have to use the longhrn-system namespace. It is mentioned in the\n# documentation of the helm chart\nhelm repo add longhorn https://charts.longhorn.io\nhelm repo update\nkubectl create namespace longhorn-system\nhelm install longhorn longhorn/longhorn --namespace longhorn-system -f values.yaml --version 1.9.1\n\n# The vpa is causing instability in the longhorn. So do not activate it for the moment\n#kubectl apply -f vpa.yml\nkubectl apply -f dashboard.yml\n\n# Apply our storage classes \n\nkubectl create secret generic longhorn-crypto --namespace longhorn-system \\\n  --from-literal=CRYPTO_KEY_VALUE=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \\\n  --from-literal=CRYPTO_KEY_PROVIDER=secret\n\nkubectl apply -f RepliccatedStorage.yaml\nkubectl apply -f UnrepliccatedStorage.yaml\n\nkubectl create secret generic longhorn-backup --namespace longhorn-system \\\n  --from-literal=CIFS_USERNAME=longhorn \\\n  --from-literal=CIFS_PASSWORD=MY_SECRET_PASSWORD\n</code></pre> <p>We have to add the disks from the UI of longhorn</p> <ul> <li>https://rpi4cluster.com/storage-setting/</li> <li>https://longhorn.io/docs/1.2.3/volumes-and-nodes/multidisk/#add-a-disk</li> </ul> <p>On how to use them take a look at https://longhorn.io/docs/1.2.2/references/examples/#block-volume</p> <p>Maybe of interest:</p> <ul> <li>https://longhorn.io/docs/1.2.2/monitoring/prometheus-and-grafana-setup/#install-longhorn-servicemonitor</li> <li>https://github.com/longhorn/longhorn/issues/1859#issuecomment-907057960 for when expanding a disk</li> </ul>"},{"location":"storage/#usefull-commands","title":"Usefull commands","text":"<pre><code>kubectl -n longhorn-system logs -f -l \"app=longhorn-manager\" --max-log-requests 10\n</code></pre>"},{"location":"storage/#resources","title":"Resources","text":"<ul> <li>https://bryanbende.com/development/2021/05/15/k3s-raspberry-pi-volumes-storage</li> <li>https://github.com/gdha/pi4-longhorn</li> <li>https://gdha.github.io/pi-stories/pi-stories9/</li> <li>https://www.jericdy.com/blog/installing-k3s-with-longhorn-and-usb-storage-on-raspberry-pi</li> <li>https://longhorn.io/docs/1.2.2/advanced-resources/volume-encryption/</li> </ul> <p>Take a look at https://longhorn.io/kb/troubleshooting-volume-with-multipath/ I had to do it in homados and aretusa, and I have to examine if it has to be done to all nodes ...</p> <pre><code># https://github.com/longhorn/longhorn/issues/1826#issuecomment-1200005051\nkubectl get snapshots.longhorn.io -n longhorn-system -l longhornvolume=pvc-1c16f507-ff8d-4b8b-aed4-7b108214618c | awk '/library-books-/{print $1}' | xargs kubectl -n longhorn-system delete snapshots.longhorn.io\n\nqemu-img convert -f raw e5ec5a95 -O vmdk torrents-settings.img\nsudo mount -o loop torrents-settings mount/\n\n# https://edoceo.com/sys/qemu\nmodprobe nbd\nqemu-nbd --connect=/dev/nbd0 disk.qcow2\n\nqemu-nbd -d /dev/nbd0\n</code></pre>"},{"location":"torrents/","title":"Torrents","text":"<p>Just a small torrent server with the necessary samba server to access the data</p> <pre><code>kubectl create namespace torrents\nkubectl apply -f pvc.yaml\nkubectl apply -f torrents.yaml\nkubectl apply -f ingressroute.yaml\n</code></pre> <p>After the deployment I should remember to add the black list  https://github.com/sayomelu/transmission-blocklist/raw/release/blocklist.gz of ips in the torrents</p>"},{"location":"torrents/#resources","title":"Resources:","text":"<ul> <li>https://github.com/sayo-melu/transmission-blocklist</li> <li>https://gist.github.com/shmup/29566c5268569069c256</li> </ul>"},{"location":"update_node/","title":"Mainting the nodes","text":"<p>NOT READY</p> <p>will have to take a look at https://longhorn.io/docs/1.2.2/volumes-and-nodes/maintenance/#updating-the-node-os-or-container-runtime</p> <p>maybe of interest  https://rpi4cluster.com/scripts/undervoltage/</p> <p>Then we can execute our playbook</p> <pre><code>ansible-galaxy collection install kubernetes.core\npip install kubernetes\nansible-playbook update_node/update_node.yml\n</code></pre> <p>// To check the k3s version https://update.k3s.io/v1-release/channels</p>"},{"location":"update_node/#check-the-use-of-deprecated-apis-from-the-programs","title":"Check the use of deprecated APIs from the programs","text":"<p>We can use kube-no-trouble :</p> <pre><code>docker pull ghcr.io/doitintl/kube-no-trouble:latest\ndocker run -it --rm -v \"${HOME}/.kube/config:/.kubeconfig\" ghcr.io/doitintl/kube-no-trouble:latest -k /.kubeconfig -t v1.33.0+k3s1\n</code></pre>"},{"location":"update_node/#usefull-commands","title":"Usefull commands:","text":"<pre><code>ansible all --forks 1 -b -m apt -a \"autoremove=yes\"\nansible aretusa -b -m apt -a \"upgrade=yes update_cache=yes\"\nansible all -b -m shell -a \"cat /var/run/reboot-required\"\nansible aretusa -m shell -a \"lsblk | grep /media/storage\"\n</code></pre>"},{"location":"update_node/#check-also","title":"Check also","text":"<p>https://github.com/weaveworks/kured</p>"},{"location":"vcs/","title":"Gitea","text":"<p>To store source code we are going to use gitea</p> <pre><code>kubectl create namespace vcs\n\nkubectl create secret generic gitea --namespace vcs \\\n  --from-literal=username=caretaker \\\n  --from-literal=password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \n\nkubectl create secret generic gitea-database --namespace vcs \\\n  --from-literal=postgres-password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64) \\\n  --from-literal=password=$(head -c 512 /dev/urandom | LC_CTYPE=C tr -cd 'a-zA-Z0-9' | head -c 64)\n\n# Follow first the steps here and use the strings from there\nkubectl create secret generic gitea-oauth --namespace vcs \\\n  --from-literal=key= ... \\\n  --from-literal= ...\n\nkubectl apply -f storage.yaml\n\nhelm repo add gitea-charts https://dl.gitea.com/charts/\nhelm repo update\nhelm install vcs gitea-charts/gitea -f values.yaml --namespace vcs --version 12.2.0\n\nkubectl apply -f ingressroute.yaml\n</code></pre>"},{"location":"vcs/#resources","title":"Resources:","text":"<ul> <li>https://gitea.com/gitea/helm-chart/issues/459</li> </ul>"},{"location":"vpa/","title":"Assing limits to the pods","text":""},{"location":"vpa/#vertical-pod-autoscaler","title":"Vertical Pod Autoscaler","text":"<p>To automate the requests and the limits of the pods we will use the  Vertical Pod Autoscaler There is no official helm repo, but we can use  an unofficial one</p> <pre><code>kubectl create namespace scaler\nhelm repo add cowboysysop https://cowboysysop.github.io/charts/\nhelm repo update\nhelm install vpa cowboysysop/vertical-pod-autoscaler --namespace scaler -f values.yaml --version v11.0.0\n</code></pre> <p>Note</p> <p>VPA can retrieve data also from Prometheus, but I do not want to couple  VPA with my monitoring stack. So for the moment I do not use prometheus  to retrieve data</p> <p>For the reason for not defining limits, take a look at:  * https://www.linkedin.com/pulse/kubernetes-make-your-services-faster-removing-cpu-limits-eric-khun/ * https://home.robusta.dev/blog/stop-using-cpu-limits</p>"},{"location":"vpa/#resources","title":"Resources:","text":"<ul> <li>https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler</li> <li>https://gist.github.com/sherifkayad/1b4e4df408e1be357168a38e1980b9a5</li> <li>https://github.com/cowboysysop/charts/tree/master/charts/vertical-pod-autoscaler</li> <li>https://artifacthub.io/packages/helm/cowboysysop/vertical-pod-autoscaler</li> <li>https://povilasv.me/vertical-pod-autoscaling-the-definitive-guide/</li> <li>https://goldilocks.docs.fairwinds.com/</li> <li>https://www.youtube.com/watch?v=UE7QX98-kO0</li> <li>https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/#create-a-limitrange-and-a-pod</li> </ul>"}]}